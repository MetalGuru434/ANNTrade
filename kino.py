# -*- coding: utf-8 -*-
""""AutoML: изображения, таблицы, тексты, временные ряды | ДЗ Pro.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10sb3dC5YVaJWNwXWm4htdcJtQfMymgvp

# Задание 1


Используем фильм с большим количеством отзывов на сайте Кинопоиск.
Возьмем фильм «Звездные войны: Последние джедаи», т.к. у него больше 100 и плохих, и хороших отзывов.
https://www.kinopoisk.ru/film/718223/

1. Сформируйте 3 отдельных датасета:
  - по 10 хороших и плохих отзывов
  - по 50 хороших и плохих отзывов
  - по 100 хороших и плохих отзывов
2. Сформируйте обучающие, валидационные и тестовые наборы данных
3. Подберите архитектуру нейронной сети для классификации собранной базы
4. Создайте таблицу с результатами

## Подготовка датасетов
"""

# Импортируем модули
import os # работа с файлами
import gdown # загрузка данных с Gdrive
import pickle # сериализация объектов (сохранение переменных в файлы)

# Скачиваем датасет по ссылке
dataset_url = 'https://storage.yandexcloud.net/aiueducation/Content/DS/L8/dataset.pickle'
gdown.download(dataset_url, quiet=True)

# Загружаем датасет в переменные со списками
with open('dataset.pickle','rb') as f:
    all_texts_good, all_texts_bad = pickle.load(f)

print(f'ХОРОШИЕ ОТЗЫВЫ ({len(all_texts_good)} штук):\n')
for t in all_texts_good[:3]:
    print(t)
    print('*'*100)

print(f'ПЛОХИЕ ОТЗЫВЫ ({len(all_texts_bad)} штук):\n')
for t in all_texts_bad[:3]:
    print(t)
    print('*'*100)

# Выполним задание с помощью циклов по требуемому количеству отзывов
for n in [10, 50, 100]:
    for category in ['good','bad']:
        dataset_category = os.path.join(str(n), category)
        os.makedirs(dataset_category, exist_ok=True)
        file_path = os.path.join(dataset_category, f'{category}.txt')
        with open(file_path, 'w', encoding='utf-8') as f:
            if category=='good':
                f.writelines(all_texts_good[:n])
            else:
                f.writelines(all_texts_bad[:n])

# Ваше решение

"""# Задание 2

1. Скачайте и распакуйте "акционный" датасет (ссылка: https://storage.yandexcloud.net/terraai/sources/shares.zip)
2. Выберите любой файл с датасетом, разделите датасет на тренировочную и проверочную выборки, найдите лучшую модель, визуализируйте график предикта.
3. Проведите серию обучений. На каждом шаге:
  а). добавляете к тренировочной выборке по N экземпляров из проверочной выборки,
  б). уменьшаете проверочную выборку на соответсвующее количество экземпляров,
  в). инициализируете новую модель и обучаете ее на тренировочной выборке из пункта а),  
  г). сохраняете N последующих значений предикта на проверочной выборке из пункта б).
  Выберите число N экспериментально, исходя из размера проверочной выборки и адекватного времени на обучение.
4. "Склейте" все полученные интервалы предикта в один массив.
5. На одном графике визуализируйте проверочную выборку, результат одной модели из п.2 и результат составного массива.  
6. Сделайте вывод
"""

# Импортируем необходимые библиотеки
import pandas as pd
import gdown

"""## Загрузка и анализ данных"""

# Загрузка обучающих данных
URL = 'https://storage.yandexcloud.net/terraai/sources/shares.zip'
download_filename = gdown.download(URL, None, quiet = True)

# Распаковка архива
!unzip -q {download_filename} -d '/content/data'

# Удаление архива
!rm -rf {download_filename}

# Возьмем "дневные" акции ГАЗПРОМа
DATASET_CSV_DIR = 'data/GAZP_1d.csv'


df = pd.read_csv(DATASET_CSV_DIR)
print(df.shape)
df

# Переформатирование датасета, исключение ненужных колонок

df = df[['<DATE>', '<OPEN>', '<HIGH>','<LOW>','<CLOSE>']]
df['<DATE>'] = pd.to_datetime(df['<DATE>'], format='%Y%m%d')
df.head()

# Ваше решение